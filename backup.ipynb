{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0004439b3fc16fce97f328916ec0ef87256d6f4576e44459c7aed1e61027bf400",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "004439b3fc16fce97f328916ec0ef87256d6f4576e44459c7aed1e61027bf400"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import pathlib\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import collections\n",
    "import tqdm\n",
    "import argparse\n",
    "\n",
    "\n",
    "def spatial(args):\n",
    "    import skimage.io\n",
    "\n",
    "    window = 224  # only to check if patch is off of boundary\n",
    " \n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    pathlib.Path(args.dest).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    raw, subtype = load_raw(args.root)\n",
    "\n",
    "    with open(args.dest + \"/subtype.pkl\", \"wb\") as f:\n",
    "        pickle.dump(subtype, f)\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    t0 = time.time()\n",
    "    section_header = None\n",
    "    gene_names = set()\n",
    "    for patient in raw:\n",
    "        for section in raw[patient]:\n",
    "            section_header = raw[patient][section][\"count\"].columns.values[0]\n",
    "            gene_names = gene_names.union(set(raw[patient][section][\"count\"].columns.values[1:]))\n",
    "    gene_names = list(gene_names)\n",
    "    gene_names.sort()\n",
    "    with open(args.dest + \"/gene.pkl\", \"wb\") as f:\n",
    "        pickle.dump(gene_names, f)\n",
    "    gene_names = [section_header] + gene_names\n",
    "    logger.info(\"Finding list of genes: \" + str(time.time() - t0))\n",
    "\n",
    "    for (i, patient) in enumerate(raw):\n",
    "        logger.info(\"Processing \" + str(i + 1) + \" / \" + str(len(raw)) + \": \" + patient)\n",
    "\n",
    "        for section in raw[patient]:\n",
    "\n",
    "            pathlib.Path(\"{}/{}/{}\".format(args.dest, subtype[patient], patient)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # This is just a blank file to indicate that the section has been completely processed.\n",
    "            # Preprocessing occassionally crashes, and this lets the preparation restart from where it let off\n",
    "            complete_filename = \"{}/{}/{}/.{}\".format(args.dest, subtype[patient], patient, section)\n",
    "            if pathlib.Path(complete_filename).exists():\n",
    "                logger.info(\"Patient {} section {} has already been processed.\".format(patient, section))\n",
    "            else:\n",
    "                logger.info(\"Processing \" + patient + \" \" + section + \"...\")\n",
    "\n",
    "                # In the original data, genes with no expression in a section are dropped from the table.\n",
    "                # This adds the columns back in so that comparisons across the sections can be done.\n",
    "                t0 = time.time()\n",
    "                missing = list(set(gene_names) - set(raw[patient][section][\"count\"].keys()))\n",
    "                c = raw[patient][section][\"count\"].values[:, 1:].astype(float)\n",
    "                pad = np.zeros((c.shape[0], len(missing)))\n",
    "                c = np.concatenate((c, pad), axis=1)\n",
    "                names = np.concatenate((raw[patient][section][\"count\"].keys().values[1:], np.array(missing)))\n",
    "                c = c[:, np.argsort(names)]\n",
    "                logger.info(\"Adding zeros and ordering columns: \" + str(time.time() - t0))\n",
    "\n",
    "                t0 = time.time()\n",
    "                count = {}\n",
    "                for (j, row) in raw[patient][section][\"count\"].iterrows():\n",
    "                    count[row.values[0]] = c[j, :]\n",
    "                logger.info(\"Extracting counts: \" + str(time.time() - t0))\n",
    "\n",
    "                t0 = time.time()\n",
    "\n",
    "\n",
    "                # tumor = {}\n",
    "                # not_int = False\n",
    "                # for (_, row) in raw[patient][section][\"tumor\"].iterrows():\n",
    "                #     if isinstance(row[1], float) or isinstance(row[2], float):\n",
    "                #         not_int = True\n",
    "                #     tumor[(int(round(row[1])), int(round(row[2])))] = (row[4] == \"tumor\")\n",
    "                # if not_int:\n",
    "                #     logger.warning(\"Patient \" + patient + \" \" + section + \" has non-integer patch coordinates.\")\n",
    "                # logger.info(\"Extracting tumors: \" + str(time.time() - t0))\n",
    "\n",
    "                t0 = time.time()\n",
    "                image = skimage.io.imread(raw[patient][section][\"image\"])\n",
    "                logger.info(\"Loading image: \" + str(time.time() - t0))\n",
    "\n",
    "                # data = []\n",
    "                for (_, row) in raw[patient][section][\"spot\"].iterrows():\n",
    "\n",
    "                    # x = int(round(row[\"pixel_x\"]))\n",
    "                    # y = int(round(row[\"pixel_y\"]))\n",
    "                    \n",
    "         \n",
    "\n",
    "                    x = int(round(float(row[0].split(',')[1])))   # coord\n",
    "                    y = int(round(float(row[0].split(',')[2])))\n",
    "\n",
    "                    spot_x = int(str(row.values[0].split(\"x\")[0])) # spot id\n",
    "                    spot_y = int(str(row.values[0].split(\"x\")[1].split(',')[0]))    \n",
    "\n",
    "\n",
    "\n",
    "                    X = image[(y + (-window // 2)):(y + (window // 2)), (x + (-window // 2)):(x + (window // 2)), :]\n",
    "\n",
    "\n",
    "                    if X.shape == (window, window, 3):\n",
    "\n",
    "                        # if (int(row[\"x\"]), int(row[\"y\"])) in tumor:\n",
    "\n",
    "                        if (str(spot_x) + \"x\" + str(spot_y)) in list(count.keys()) :\n",
    "                            # data.append((X,\n",
    "                            #              count[str(int(row[\"x\"])) + \"x\" + str(int(row[\"y\"]))],\n",
    "                            #             #  tumor[(int(row[\"x\"]), int(row[\"y\"]))],\n",
    "                            #              np.array([x, y]),\n",
    "                            #              np.array([patient]),\n",
    "                            #              np.array([section]),\n",
    "                            #              np.array([int(row[\"x\"]), int(row[\"y\"])]),\n",
    "                            #              ))\n",
    "                            filename = \"{}/{}/{}/{}_{}_{}.npz\".format(args.dest, subtype[patient], patient, section,\n",
    "                                                                      spot_x, spot_y)\n",
    "                            np.savez_compressed(filename, count=count[str(spot_x) + \"x\" + str(spot_y)],\n",
    "                                                # tumor=tumor[(int(row[\"x\"]), int(row[\"y\"]))],\n",
    "                                                pixel=np.array([x, y]),\n",
    "                                                patient=np.array([patient]),\n",
    "                                                section=np.array([section]),\n",
    "                                                index=np.array([spot_x, spot_y]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            logger.warning(\"Patch \" + str(spot_x) + \"x\" + str(spot_y) + \" not found in \" + patient + \" \" + section)\n",
    "                    else:\n",
    "                        logger.warning(\"Detected spot too close to edge.\")\n",
    "                logger.info(\"Saving patches: \" + str(time.time() - t0))\n",
    "\n",
    "                with open(complete_filename, \"w\"):\n",
    "                    pass\n",
    "    logger.info(\"Preprocessing took \" + str(time.time() - t) + \" seconds\")\n",
    "\n",
    "    if (not os.path.isfile(\"data/hist2tscript-patch/mean_expression.npy\") or\n",
    "        not os.path.isfile(\"data/hist2tscript-patch/median_expression.npy\")):\n",
    "\n",
    "\n",
    "        logging.info(\"Computing statistics of dataset\")\n",
    "        gene = []\n",
    "        for filename in tqdm.tqdm(glob.glob(\"{}/*/*/*_*_*.npz\".format(args.dest))):\n",
    "            npz = np.load(filename)\n",
    "            count = npz[\"count\"]\n",
    "            gene.append(np.expand_dims(count, 1))\n",
    "\n",
    "\n",
    "        gene = np.concatenate(gene, 1)\n",
    "\n",
    "        logging.info( \"There are {} genes and {} spots in total for all the patients and sections.\".format(gene.shape[0], gene.shape[1]))\n",
    "\n",
    "        np.save( \"data/hist2tscript-patch/mean_expression.npy\", np.mean(gene, 1))\n",
    "        np.save(\"data/hist2tscript-patch/median_expression.npy\", np.median(gene, 1))\n",
    "\n",
    "\n",
    "def newer_than(file1, file2):\n",
    "    \"\"\"\n",
    "    Returns True if file1 is newer than file2.\n",
    "    A typical use case is if file2 is generated using file1.\n",
    "    For example:\n",
    "\n",
    "    if newer_than(file1, file2):\n",
    "        # update file2 based on file1\n",
    "    \"\"\"\n",
    "    return os.path.isfile(file1) and (not os.path.isfile(file2) or os.path.getctime(file1) > os.path.getctime(file2))\n",
    "\n",
    "\n",
    "def load_section(root: str, patient: str, section: str, subtype: str):\n",
    "    \"\"\"\n",
    "    Loads data for one section of a patient.\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    import gzip\n",
    "\n",
    "    file_root = root + \"/\" + subtype + \"/\" + patient + \"/\" + patient + \"_\" + section\n",
    "\n",
    "    # image = skimage.io.imread(file_root + \".jpg\")\n",
    "    image = file_root + \".jpg\"\n",
    "\n",
    "    if newer_than(file_root + \".tsv.gz\", file_root + \".pkl\"):\n",
    "        with gzip.open(file_root + \".tsv.gz\", \"rb\") as f:\n",
    "            count = pandas.read_csv(f, sep=\"\\t\")\n",
    "        with open(file_root + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(count, f)\n",
    "    else:\n",
    "        with open(file_root + \".pkl\", \"rb\") as f:\n",
    "            count = pickle.load(f)\n",
    "\n",
    "    if newer_than(file_root + \".spots.gz\", file_root + \".spots.pkl\"):\n",
    "        spot = pandas.read_csv(file_root + \".spots.gz\", sep=\"\\t\")\n",
    "        with open(file_root + \".spots.pkl\", \"wb\") as f:\n",
    "            pickle.dump(spot, f)\n",
    "    else:\n",
    "        with open(file_root + \".spots.pkl\", \"rb\") as f:\n",
    "            spot = pickle.load(f)\n",
    "\n",
    "\n",
    "    return {\"image\": image, \"count\": count, \"spot\": spot}\n",
    "\n",
    "\n",
    "def load_raw(root: str):\n",
    "    \"\"\"\n",
    "    Loads data for all patients.\n",
    "    \"\"\"\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Wildcard search for patients/sections\n",
    "\n",
    "    images = glob.glob(root + \"/*/*/*_*.jpg\")\n",
    "\n",
    "    #   file_root = root + \"/\" + subtype + \"/\" + patient + \"/\" + patient + \"_\" + section\n",
    " \n",
    "\n",
    "\n",
    "    # data/hist2tscript/HER2nonluminal/BC23567/    HE_BC23567_E2.jpg\n",
    "\n",
    "    # Dict mapping patient ID (str) to a list of all sections available for the patient (List[str])  sections: C/D\n",
    "    patient = collections.defaultdict(list)\n",
    "    for (p, s) in map(lambda x: x.split(\"/\")[-1][:-4].split(\"_\"), images):\n",
    "        patient[p].append(s)\n",
    "\n",
    "    # Dict mapping patient ID (str) to subtype (str)\n",
    "    subtype = {}\n",
    "    for (st, p) in map(lambda x: (x.split(\"/\")[-3], x.split(\"/\")[-1][:-4].split(\"_\")[0]), images):\n",
    "        if p in subtype:\n",
    "            if subtype[p] != st:\n",
    "                raise ValueError(\"Patient {} is the same marked as type {} and {}.\".format(p, subtype[p], st))\n",
    "        else:\n",
    "            subtype[p] = st\n",
    "\n",
    "    logger.info(\"Loading raw data...\")\n",
    "    t = time.time()\n",
    "    data = {}\n",
    "    with tqdm.tqdm(total=sum(map(len, patient.values()))) as pbar:\n",
    "        for p in patient:\n",
    "            data[p] = {}\n",
    "            for s in patient[p]:\n",
    "                data[p][s] = load_section(root, p, s, subtype[p])\n",
    "                pbar.update()\n",
    "    logger.info(\"Loading raw data took \" + str(time.time() - t) + \" seconds.\")\n",
    "\n",
    "    return data, subtype\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process the paths.')\n",
    "\n",
    "parser.add_argument('--root',  type=str, default='data/hist2tscript/',\n",
    "                    help='an integer for the accumulator')     \n",
    "parser.add_argument('--dest',  type=str, default='data/hist2tscript-patch/',\n",
    "                    help='an integer for the accumulator')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "spatial(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2748de088d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "np.zeros((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "np.zeros((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1., 0., 2., 0.],\n",
    "              [0., 0., 0., 9.],\n",
    "              [0., 0., 0., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3., 9., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "np.sum(t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ True,  True, False])"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "np.sum(t, 1)!=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ((np.sum(t, 1)!=0)*1 + ((np.sum(np.array(t!=0),1))>=(0.1*t.shape[1]))*1)==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 2., 0.],\n",
       "       [0., 0., 0., 9.]])"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "t[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/chenxingjian/PycharmProjects/mnt/mnt_project/FromZerotoOne/data/hist2tscript-patch/gene.pkl\",\"rb\") as f:   \n",
    "    gene = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2, 1, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "(np.sum(np.array(t!=0),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ True,  True, False])"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "((np.sum(np.array(t!=0),1))>=(0.1*t.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "((np.sum(np.array(t!=0),1))>=(0.1*t.shape[1]))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 2., 0.],\n",
       "       [0., 0., 0., 9.]])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "t[((np.sum(np.array(t!=0),1))>=(0.1*t.shape[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "((np.sum(np.array(t!=0),1))!=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "list(np.nonzero(np.sum(np.array(t!=0),1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import logging\n",
    "import pathlib\n",
    "import traceback\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import socket\n",
    "import argparse\n",
    "import collections\n",
    "import utils\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_spatial(args=None):\n",
    "\n",
    "    \n",
    "        ### Seed ###\n",
    "        random.seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "\n",
    "        ### Select device for computation ###\n",
    "        device = (\"cuda\" if args.gpu else \"cpu\")\n",
    "\n",
    "        ### Split patients into folds ###\n",
    "        patient = get_spatial_patients()\n",
    "        train_patients = []\n",
    "        test_patients = []\n",
    "        for (i, p) in enumerate(patient):\n",
    "            for s in patient[p]:\n",
    "                if p in args.testpatients or (p, s) in args.testpatients:\n",
    "                    test_patients.append((p, s))\n",
    "                else:\n",
    "                    train_patients.append((p, s))\n",
    "\n",
    "        ### Dataset setup ###  \n",
    "\n",
    "        window = args.window\n",
    "\n",
    "\n",
    "\n",
    "        # here need to be changed for split training data and test data\n",
    "\n",
    "        train_dataset = utils.Spatial(train_patients, window=window, gene_filter=args.gene_filter, \n",
    "                transform=torchvision.transforms.ToTensor())\n",
    "        \n",
    "        # print(len(train_dataset)) #29678\n",
    "\n",
    "\n",
    "\n",
    "        train_size = int(0.9 * len(train_dataset))\n",
    "        val_size = len(train_dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch, \n",
    "                                    num_workers=args.workers, shuffle=True, pin_memory=args.gpu)\n",
    "\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch, \n",
    "                                    num_workers=args.workers, shuffle=True, pin_memory=args.gpu)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Estimate mean and covariance\n",
    "        t = time.time()\n",
    "        n_samples = 10\n",
    "        mean = 0.\n",
    "        std = 0.\n",
    "        n = 0\n",
    "        for (i, (X, *_)) in enumerate(train_loader):\n",
    "            X = X.transpose(0, 1).contiguous().view(3, -1)\n",
    "            n += X.shape[1]\n",
    "            mean += torch.sum(X, dim=1)\n",
    "            std += torch.sum(X ** 2, dim=1)\n",
    "            if i > n_samples:\n",
    "                break\n",
    "        mean /= n\n",
    "        std = torch.sqrt(std / n - mean ** 2)\n",
    "        print(\"Estimating mean (\" + str(mean) + \") and std (\" + str(std) + \" took \" + str(time.time() - t) + 's')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Transform and data argumentation (TODO: spatial and multiscale ensemble)\n",
    "\n",
    "        transform = []\n",
    "        transform.extend([torchvision.transforms.RandomHorizontalFlip(),\n",
    "                          torchvision.transforms.RandomVerticalFlip(),\n",
    "                          torchvision.transforms.RandomApply([torchvision.transforms.RandomRotation((90, 90))]),\n",
    "                          torchvision.transforms.ToTensor(),\n",
    "                          torchvision.transforms.Normalize(mean=mean, std=std)])\n",
    "        transform = torchvision.transforms.Compose(transform)\n",
    "        # for training data\n",
    "        train_dataset.transform = transform\n",
    "\n",
    "        # for val data\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                        torchvision.transforms.Normalize(mean=mean, std=std)])\n",
    "        val_dataset.transform = transform\n",
    "\n",
    "        # for test data\n",
    "        if args.average:\n",
    "            transform = torchvision.transforms.Compose([utils.transforms.EightSymmetry(),\n",
    "                                                        torchvision.transforms.Lambda(lambda symmetries: torch.stack([torchvision.transforms.Normalize(mean=mean, std=std)(torchvision.transforms.ToTensor()(s)) for s in symmetries]))])\n",
    "        else:\n",
    "            transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                        torchvision.transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "        test_dataset = utils.Spatial(test_patients, transform, window=args.window, gene_filter=args.gene_filter)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch, num_workers=args.workers, shuffle=True, pin_memory=args.gpu)\n",
    "\n",
    "\n",
    "        # Find number of required outputs\n",
    "\n",
    "        outputs = train_dataset[0][1].shape[0]\n",
    "           \n",
    "\n",
    "        # ### Model setup ###\n",
    "        # model = torchvision.models.__dict__[args.model](pretrained=args.pretrained)\n",
    "        start_epoch = 0\n",
    "        # ###Changes number of outputs for the model, return model###\n",
    "        # utils.nn.set_out_features(model, outputs)\n",
    "        # if args.gpu:\n",
    "        #     model = torch.nn.DataParallel(model)\n",
    "        # model.to(device)\n",
    "\n",
    "        # ### Optimizer setup ###\n",
    "        # # chose the parameters that need to be optimized  #\n",
    "        # parameters = utils.nn.get_finetune_parameters(model, args.finetune, args.randomize)\n",
    "\n",
    "\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        feature = model._fc.in_features\n",
    "        model._fc = torch.nn.Linear(in_features=feature,out_features=outputs)\n",
    "\n",
    "        # fix all the layers before fc layer, set model[-1]\n",
    "        # print(model)\n",
    "        # for parameter in model.parameters():\n",
    "        #     parameter.requires_grad = False\n",
    "        # for parameter in model.fc.parameters():\n",
    "        #     parameter.requires_grad = True\n",
    "        model.to(device)\n",
    "        # parameters = utils.nn.get_finetune_parameters(model, args.finetune, args.randomize)\n",
    "        optim = torch.optim.__dict__[args.optim](model._fc.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "\n",
    "        # Compute mean expression as baseline\n",
    "        \n",
    "        # t = time.time()\n",
    "        # mean_expression = torch.zeros(train_dataset[0][1].shape)\n",
    "       \n",
    "        # for (i, (_, gene, *_)) in enumerate(train_loader):\n",
    "        #     print(\"{:8d} / {:d}:    {:4.0f} / {:4.0f} seconds\".format(i + 1, len(train_loader), time.time() - t, (time.time() - t) * len(train_loader) / (i + 1)), end=\"\\r\", flush=True)\n",
    "\n",
    "        #     mean_expression += torch.sum(gene, 0)/gene.shape[0]\n",
    "        # mean_expression /= len(train_loader)\n",
    "        # mean_expression = mean_expression.to(device)\n",
    "        # print(\"Computing mean expression took {}\".format(time.time() - t))\n",
    "\n",
    "\n",
    "\n",
    "        ### Training Loop ###   save for every epoch but actually we just need to save the last one\n",
    "        # save the best model and npz file\n",
    "\n",
    "        for epoch in range(start_epoch, args.epochs):\n",
    "            print(\"Epoch #\" + str(epoch + 1))\n",
    "\n",
    "            # for each epoch, loop train, val and test\n",
    "\n",
    "            for (dataset, loader) in [(\"train\", train_loader), (\"val\", val_loader), (\"test\", test_loader)]:\n",
    "\n",
    "                t = time.time()\n",
    "\n",
    "                if dataset == \"train\":\n",
    "                    torch.set_grad_enabled(True)\n",
    "                    model.train()\n",
    "                else:\n",
    "                    torch.set_grad_enabled(False)\n",
    "                    model.eval()\n",
    "\n",
    "\n",
    "\n",
    "                total = 0\n",
    "                total_mean = 0\n",
    "                n=0\n",
    "                genes = []\n",
    "                predictions = []\n",
    "                counts = []\n",
    "                coord = []\n",
    "                patient = []\n",
    "                section = []\n",
    "                pixel = []\n",
    "\n",
    "\n",
    "                print(dataset + \":\")\n",
    "                for (i, (X, gene, c, ind, pat, s, pix)) in enumerate(loader):\n",
    "                    \n",
    "                    print(\"********\",i)\n",
    "\n",
    "\n",
    "                    counts.append(gene.detach().numpy())\n",
    "                    coord.append(c.detach().numpy())\n",
    "                    patient += pat\n",
    "                    section += s\n",
    "                    pixel.append(pix.detach().numpy())\n",
    "\n",
    "                    X = X.to(device)\n",
    "                    gene = gene.to(device)\n",
    "\n",
    "                    if dataset == \"test\" and args.average:\n",
    "                        batch, n_sym, c, h, w = X.shape\n",
    "                        X = X.view(-1, c, h, w)\n",
    "                    \n",
    "\n",
    "                    pred = model(X)  # [32, 5943])\n",
    "\n",
    "                    if dataset == \"test\" and args.average:\n",
    "                        pred = pred.view(batch, n_sym, -1).mean(1)\n",
    "\n",
    "\n",
    "\n",
    "                    predictions.append(pred.cpu().detach().numpy())\n",
    "                    loss = torch.sum((pred - gene) ** 2) / outputs # in a epoch, one batch average gene loss\n",
    "\n",
    "                    # print(loss/gene.shape[0])\n",
    "\n",
    "                    total += loss.cpu().detach().numpy()\n",
    "                    n += gene.shape[0]  #[32, 250]\n",
    "\n",
    "\n",
    "                    message = \"\"\n",
    "                    message += \"Batch: {:8d} / {:d} ({:4.0f} / {:4.0f}):\".format(i + 1, len(loader), time.time() - t, (time.time() - t) * len(loader) / (i + 1))\n",
    "                    message += \"    Batch-based Loss={:.9f}\".format(total / n)  # loop each batch, print averaged batch-gene loss\n",
    "                    print(message)\n",
    "\n",
    "                      \n",
    "                    if dataset == \"train\" :\n",
    "                        optim.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "\n",
    "\n",
    "\n",
    "                print(\"    Epoch-based Loss:       \" + str(total / len(loader.dataset)))\n",
    "     \n",
    "                # one epoch finished, and for the last is test loop, we save\n",
    "                predictions = np.concatenate(predictions)\n",
    "                counts = np.concatenate(counts)\n",
    "                coord = np.concatenate(coord)\n",
    "                pixel = np.concatenate(pixel)\n",
    "                # me = mean_expression.cpu().numpy(),  # this is training mean_expression\n",
    "    #                   \n",
    "\n",
    "                pathlib.Path(os.path.dirname(args.pred_root)).mkdir(parents=True, exist_ok=True)\n",
    "                np.savez_compressed(args.pred_root + str(epoch + 1),\n",
    "                                    task=\"gene\",\n",
    "                                    counts=counts,\n",
    "                                    predictions=predictions,\n",
    "                                    coord=coord,\n",
    "                                    patient=patient,\n",
    "                                    section=section,\n",
    "                                    pixel=pixel,\n",
    "                                    # mean_expression=me,\n",
    "                                    ensg_names=test_dataset.ensg_names,\n",
    "                                    gene_names=test_dataset.gene_names,\n",
    "                )\n",
    "\n",
    "\n",
    "                # Saving after test so that if information from test is needed, they will not get skipped\n",
    "                # if dataset == \"test\" and args.checkpoint is not None and ((epoch + 1) % args.checkpoint_every) == 0 and args.model != \"rf\":\n",
    "                #     pathlib.Path(os.path.dirname(args.checkpoint)).mkdir(parents=True, exist_ok=True)\n",
    "                  \n",
    "\n",
    "                #     torch.save({\n",
    "                #         'model': model.state_dict(),\n",
    "                #         'optim' : optim.state_dict(),\n",
    "                #     }, args.checkpoint + str(epoch + 1) + \".pt\")\n",
    "\n",
    "                #     if epoch != 0 and (args.keep_checkpoints is None or (epoch + 1 - args.checkpoint_every) not in args.keep_checkpoints):\n",
    "                #         os.remove(args.checkpoint + str(epoch + 1 - args.checkpoint_every) + \".pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_spatial_patients():\n",
    "    \"\"\"\n",
    "    Returns a dict of patients to sections.\n",
    "\n",
    "    The keys of the dict are patient names (str), and the values are lists of\n",
    "    section names (str).\n",
    "    \"\"\"\n",
    "    patient_section = map(lambda x: x.split(\"/\")[-1].split(\".\")[0].split(\"_\"), glob.glob(\"data/hist2tscript/*/*/*.jpg\"))\n",
    "    patient = collections.defaultdict(list)\n",
    "    for (p, s) in patient_section:\n",
    "        patient[p].append(s)\n",
    "    return patient\n",
    "\n",
    "\n",
    "def patient_or_section(name):\n",
    "        if \"_\" in name:\n",
    "            return tuple(name.split(\"_\"))\n",
    "        return name\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process the paths.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--seed', '-s', type=int, default=0, help='RNG seed')\n",
    "parser.add_argument(\"--gpu\", action=\"store_true\", help=\"use GPU\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--testpatients\", nargs=\"*\", type=patient_or_section, default=None,\n",
    "                                   help=\"all the rest patients will be used as the training data\")\n",
    "parser.add_argument(\"--window\", type=int, default=224, help=\"window size\")\n",
    "parser.add_argument(\"--gene_filter\", choices=[\"none\", \"high\", 250], default=250,\n",
    "                       help=\"special gene filters\")\n",
    "parser.add_argument(\"--batch\", type=int, default=256, help=\"training batch size\")    \n",
    "parser.add_argument(\"--workers\", type=int, default=4, help=\"number of workers for dataloader\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--model\", \"-m\", default=\"vgg11\",\n",
    "                        # choices=sorted(name for name in torchvision.models.__dict__ if name.islower() and not name.startswith(\"__\") and callable(torchvision.models.__dict__[name])),  TODO: autocomplete speed issue\n",
    "                        help=\"model architecture\")\n",
    "parser.add_argument(\"--pretrained\", action=\"store_true\",\n",
    "                    help=\"use ImageNet pretrained weights\")\n",
    "parser.add_argument(\"--finetune\", type=int, nargs=\"?\", const=1, default=None,\n",
    "                             help=\"fine tune last n layers\")\n",
    "parser.add_argument(\"--randomize\", action=\"store_true\",\n",
    "                                   help=\"randomize weights in layers to be fined tuned\")\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--optim\", default=\"SGD\",\n",
    "                        # choices=sorted(name for name in torchvision.models.__dict__ if name.islower() and not name.startswith(\"__\") and callable(torchvision.models.__dict__[name])),  TODO: autocomplete speed issue and change to optim instead of model\n",
    "                        help=\"optimizer\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"learning rate\")\n",
    "parser.add_argument(\"--momentum\", type=float, default=0.9, help=\"momentum for SGD\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0, help=\"weight decay for SGD\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=100, help=\"number of epochs\")\n",
    "\n",
    "\n",
    "parser.add_argument(\"--average\", action=\"store_true\", help=\"average between rotations and reflections\")\n",
    "parser.add_argument(\"--pred_root\", type=str, default=None, help=\"root for prediction outputs\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "run_spatial(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9193314819335937\n1.0168716430664062\n0.87110302734375\n0.8447691040039063\n0.99274658203125\n0.9404971313476562\n0.9094579467773437\n0.9560597534179688\n0.830826416015625\n0.86204931640625\n0.9589568481445313\n0.8984053955078125\n0.9861575317382812\n0.902651611328125\n0.8830653686523438\n0.88997119140625\n0.86878125\n0.950525146484375\n0.9787474975585938\n0.9335302734375\n0.88711865234375\n0.9798858642578125\n0.9214951171875\n0.9374384765625\n1.0119865112304687\n0.9349647216796875\n0.902815185546875\n0.94776904296875\n0.87917236328125\n0.5570457763671876\n0.0\n0.0\n"
     ]
    }
   ],
   "source": [
    "a=np.load('BC23287_1.npz')\n",
    "b=a['counts']\n",
    "c=a['predictions']\n",
    "for i in range(batch):\n",
    "    print(np.sum((b[i*batch:(i+1)*batch,:] - c[i*batch:(i+1)*batch,:]) ** 2)/(250*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.06476171875\n1.0140812377929687\n1.1718956298828125\n1.0427177734375\n0.9164627075195313\n1.0842373046875\n1.062562744140625\n1.1283795166015624\n0.9940906982421875\n1.124470703125\n1.07689453125\n0.9617781982421875\n1.077207763671875\n1.06529736328125\n1.157640625\n1.09458984375\n1.0609080810546876\n1.12093408203125\n0.9850051879882813\n1.064634765625\n1.013691650390625\n1.026901123046875\n1.17226220703125\n0.9928989868164062\n1.0620875244140624\n0.948853759765625\n1.1373251953125\n1.0010823974609375\n0.9962522583007812\n0.67506689453125\n0.0\n0.0\n"
     ]
    }
   ],
   "source": [
    "a=np.load('/home/chenxingjian/PycharmProjects/mnt/mnt_project/FromZerotoOne/output/1.npz')\n",
    "b=a['counts']\n",
    "c=a['predictions']\n",
    "for i in range(32):\n",
    "    print(np.sum((b[i*32:(i+1)*32,:] - c[i*32:(i+1)*32,:]) ** 2)/(250*32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0574857444561774"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "import torch\n",
    "np.sum((b - c) ** 2) / (250*947)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a['counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=a['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(947, 250)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 32\n0.9193314819335937\n32 64\n1.0168716430664062\n64 96\n0.87110302734375\n96 128\n0.8447691040039063\n128 160\n0.99274658203125\n160 192\n0.9404971313476562\n192 224\n0.9094579467773437\n224 256\n0.9560597534179688\n256 288\n0.830826416015625\n288 320\n0.86204931640625\n320 352\n0.9589568481445313\n352 384\n0.8984053955078125\n384 416\n0.9861575317382812\n416 448\n0.902651611328125\n448 480\n0.8830653686523438\n480 512\n0.88997119140625\n512 544\n0.86878125\n544 576\n0.950525146484375\n576 608\n0.9787474975585938\n608 640\n0.9335302734375\n640 672\n0.88711865234375\n672 704\n0.9798858642578125\n704 736\n0.9214951171875\n736 768\n0.9374384765625\n768 800\n1.0119865112304687\n800 832\n0.9349647216796875\n832 864\n0.902815185546875\n864 896\n0.94776904296875\n896 928\n0.87917236328125\n928 960\n0.5570457763671876\n960 992\n0.0\n992 1024\n0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print(i*32,(i+1)*32)\n",
    "    print(np.sum((b[i*32:(i+1)*32,:] - c[i*32:(i+1)*32,:]) ** 2)/(250*32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9243233896515312"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "import torch\n",
    "np.sum((b - c) ** 2) / (250*947)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9243233896515312"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "875.33425/947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}